import streamlit as st
from transformers import pipeline
import os
from PIL import Image

# --- Configuration de la Page Streamlit ---
st.set_page_config(
    page_title="ü§ñ Chatbot L√©ger Multi-Capacit√©s (Open Source)",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("üß† Assistant G√©n√©rateur de Texte et Code L√©ger")
st.caption("Propuls√© par le mod√®le **GPT-2** (Hugging Face) pour la compatibilit√© avec les plateformes gratuites (CPU/m√©moire limit√©e).")
st.divider()

# --- Configuration et Chargement du Mod√®le (GPT-2) ---

@st.cache_resource
def load_generator():
    """Charge le pipeline du mod√®le GPT-2 (mis en cache)."""
    # NOTE: Remplacer "gpt2" par un mod√®le plus puissant n√©cessite plus de ressources (GPU/RAM)
    print("Chargement du mod√®le GPT-2...")
    generator = pipeline("text-generation", model="gpt2")
    return generator

# Charger le mod√®le une seule fois au d√©marrage
generator = load_generator()


# --- Fonctions pour la G√©n√©ration d'Image (Simul√©e) ---
# *La vraie* g√©n√©ration d'image est impossible sur CPU/plateformes gratuites.
# Cette fonction simule la capacit√© pour maintenir la structure du chatbot.

def generer_image(prompt_image, output_filename):
    """Simule la g√©n√©ration d'image et cr√©e une image placeholder simple."""
    try:
        img = Image.new('RGB', (512, 512), color = 'red')
        
        # Ajouter un texte au centre
        from PIL import ImageDraw, ImageFont
        d = ImageDraw.Draw(img)
        
        # Essayer de charger une police par d√©faut
        try:
            font = ImageFont.truetype("arial.ttf", 30)
        except IOError:
            font = ImageFont.load_default()
            
        d.text((10,10), f"Image (CPU Only): {prompt_image}\n[Simul√©e, n√©cessite un GPU puissant]", fill=(255,255,0), font=font)
        img.save(output_filename)
        return True
    except Exception as e:
        st.error(f"Erreur de simulation d'image : {e}")
        return False

# --- Logique du Chatbot ---

# Initialiser l'historique de la conversation
if "messages" not in st.session_state:
    st.session_state.messages = []
    # Message de bienvenue initial
    st.session_state.messages.append({"role": "assistant", "content": "Bonjour ! Je suis bas√© sur GPT-2, capable de g√©n√©rer du texte et du code (utilisez la commande `!image` pour tester la capacit√© d'image simul√©e)." })

# Afficher les messages pr√©c√©dents
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Gestion de la nouvelle entr√©e utilisateur
if prompt := st.chat_input("Dites bonjour ou demandez '√âcris une fonction Python...'"):
    # 1. Ajouter le message utilisateur
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. Pr√©parer la r√©ponse du bot
    with st.chat_message("assistant"):
        with st.spinner("L'IA g√©n√®re la r√©ponse..."):
            
            # --- LOGIQUE DU CHATBOT (Capacit√©s L√©g√®res) ---

            if prompt.lower().startswith("!image"):
                # Capacit√©: G√©n√©ration d'Image (simul√©e)
                prompt_image = prompt[6:].strip()
                filename = "temp_image_output.png"
                
                st.text(f"üé® Simulation de la g√©n√©ration d'image pour : {prompt_image}")
                
                if generer_image(prompt_image, filename):
                    st.image(filename, caption=f"Image Simul√©e pour : {prompt_image}")
                    st.success("La g√©n√©ration d'image a √©t√© simul√©e avec succ√®s. La vraie version n√©cessite un GPU.")
                    r√©ponse_finale = "Voici l'image que j'ai cr√©√©e (simulation CPU)."
                    
                    # Nettoyage
                    os.remove(filename) 
                else:
                    r√©ponse_finale = "√âchec de la simulation d'image."

            else:
                # Capacit√©: Texte/Code/Chat (via GPT-2)
                try:
                    response_text = generator(
                        prompt,
                        max_length=250,  # Longueur maximale
                        num_return_sequences=1,
                        do_sample=True,
                        temperature=0.7 # Temp√©rature pour un peu de cr√©ativit√©
                    )[0]['generated_text']

                    # Nettoyer la r√©ponse pour enlever l'√©cho du prompt
                    if response_text.startswith(prompt):
                        r√©ponse_finale = response_text[len(prompt):].strip()
                    else:
                        r√©ponse_finale = response_text
                        
                except Exception as e:
                    st.error(f"Erreur de g√©n√©ration de texte : {e}")
                    r√©ponse_finale = "D√©sol√©, une erreur est survenue lors de la g√©n√©ration de la r√©ponse textuelle."


        # 3. Affichage et sauvegarde de la r√©ponse
        st.markdown(r√©ponse_finale)
        st.session_state.messages.append({"role": "assistant", "content": r√©ponse_finale})

import streamlit as st
from transformers import pipeline
from PIL import Image, ImageDraw, ImageFont
import io

# -----------------------------------------------------
# CONFIGURATION DE LA PAGE
# -----------------------------------------------------
st.set_page_config(
    page_title="Assistant IA L√©ger ‚Äì distilGPT-2",
    layout="wide",
)

# -----------------------------------------------------
# TITRE ET DESCRIPTION
# -----------------------------------------------------
st.title("Assistant IA L√©ger ‚Äì distilGPT-2")
st.caption("Chatbot de g√©n√©ration de texte et simulation d‚Äôimages ‚Äì compatible Streamlit Cloud (CPU uniquement).")
st.divider()

# -----------------------------------------------------
# CHARGEMENT DU MOD√àLE (DISTILGPT-2)
# -----------------------------------------------------
@st.cache_resource
def load_model():
    try:
        generator = pipeline("text-generation", model="distilgpt2")
        return generator
    except Exception as e:
        st.error(f"Erreur de chargement du mod√®le : {e}")
        return None

generator = load_model()

# -----------------------------------------------------
# FONCTION DE G√âN√âRATION D‚ÄôIMAGE SIMUL√âE
# -----------------------------------------------------
def generer_image(prompt_image: str):
    """Cr√©e une image simul√©e directement en m√©moire."""
    img = Image.new('RGB', (512, 512), color=(40, 40, 65))
    draw = ImageDraw.Draw(img)

    try:
        font = ImageFont.truetype("arial.ttf", 24)
    except IOError:
        font = ImageFont.load_default()

    texte = f"Simulation :\n{prompt_image[:90]}..."
    draw.text((20, 230), texte, fill=(255, 255, 100), font=font)

    img_bytes = io.BytesIO()
    img.save(img_bytes, format="PNG")
    img_bytes.seek(0)
    return img_bytes

# -----------------------------------------------------
# INITIALISATION DU CHAT
# -----------------------------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.memory = []
    st.session_state.messages.append({
        "role": "assistant",
        "content": "Bonjour ! Je suis un assistant IA l√©ger bas√© sur distilGPT-2.\n"
                   "Utilisez '!image <description>' pour g√©n√©rer une image simul√©e."
    })

# -----------------------------------------------------
# AFFICHAGE DE L‚ÄôHISTORIQUE
# -----------------------------------------------------
def afficher_historique():
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

afficher_historique()

# -----------------------------------------------------
# ENTR√âE UTILISATEUR
# -----------------------------------------------------
if prompt := st.chat_input("Entrez votre message ici..."):
    # Ajouter le message utilisateur
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Afficher imm√©diatement le message
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # G√©n√©rer la r√©ponse du bot
    with st.chat_message("assistant"):
        with st.spinner("L'IA r√©fl√©chit..."):
            r√©ponse_finale = ""

            # Commande : !image
            if prompt.lower().startswith("!image"):
                prompt_image = prompt[6:].strip() or "Aucune description"
                st.info(f"Simulation d'image pour : {prompt_image}")
                img_bytes = generer_image(prompt_image)
                st.image(img_bytes, caption=f"Image simul√©e : {prompt_image}")
                r√©ponse_finale = "Voici une image simul√©e (version CPU)."

            # Commande : !m√©moire
            elif prompt.lower().startswith("!m√©moire"):
                m√©moire_text = "\n".join(
                    [f"- {m}" for m in st.session_state.memory[-5:]]
                ) or "M√©moire vide."
                r√©ponse_finale = f"Derniers sujets :\n{m√©moire_text}"

            # R√©ponse textuelle via distilGPT-2
            elif generator:
                try:
                    contexte = " ".join(st.session_state.memory[-2:]) + " " + prompt
                    result = generator(
                        contexte,
                        max_length=100,
                        num_return_sequences=1,
                        temperature=0.8,
                        top_k=50,
                        top_p=0.9,
                        do_sample=True
                    )[0]['generated_text']

                    # Nettoyer l‚Äô√©cho du prompt
                    if result.startswith(prompt):
                        result = result[len(prompt):].strip()

                    r√©ponse_finale = result
                    st.session_state.memory.append(prompt)
                except Exception as e:
                    r√©ponse_finale = f"Erreur pendant la g√©n√©ration : {e}"
            else:
                r√©ponse_finale = "Le mod√®le n‚Äôa pas pu √™tre charg√©."

        # Ajouter la r√©ponse √† l'historique et l'afficher
        st.session_state.messages.append({"role": "assistant", "content": r√©ponse_finale})
        st.markdown(r√©ponse_finale)

# -----------------------------------------------------
# PIED DE PAGE
# -----------------------------------------------------
st.divider()
st.markdown("""
<div style='text-align:center; color:gray; font-size:0.9em;'>
Propuls√© par distilGPT-2 | Compatible Streamlit Cloud | CPU uniquement
</div>
""", unsafe_allow_html=True)# G√âN√âRATION D‚ÄôIMAGE SIMUL√âE
# -----------------------------------------------------
def generer_image(prompt_image: str):
    """Cr√©e une image simul√©e directement en m√©moire."""
    img = Image.new('RGB', (512, 512), color=(40, 40, 65))
    draw = ImageDraw.Draw(img)

    try:
        font = ImageFont.truetype("arial.ttf", 24)
    except IOError:
        font = ImageFont.load_default()

    texte = f"Simulation :\n{prompt_image[:90]}..."
    draw.text((20, 230), texte, fill=(255, 255, 100), font=font)

    img_bytes = io.BytesIO()
    img.save(img_bytes, format="PNG")
    img_bytes.seek(0)
    return img_bytes

# -----------------------------------------------------
# INITIALISATION DU CHAT
# -----------------------------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.memory = []
    st.session_state.messages.append({
        "role": "assistant",
        "content": "Bonjour ! Je suis un assistant IA l√©ger bas√© sur distilGPT-2.\n"
                   "Utilisez '!image <description>' pour g√©n√©rer une image simul√©e."
    })

# -----------------------------------------------------
# AFFICHAGE DE L‚ÄôHISTORIQUE
# -----------------------------------------------------
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# -----------------------------------------------------
# ENTR√âE UTILISATEUR
# -----------------------------------------------------
if prompt := st.chat_input("Entrez votre message ici..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("L'IA r√©fl√©chit..."):
            r√©ponse_finale = ""

            # Commande : !image
            if prompt.lower().startswith("!image"):
                prompt_image = prompt[6:].strip() or "Aucune description"
                st.info(f"Simulation d'image pour : {prompt_image}")
                img_bytes = generer_image(prompt_image)
                st.image(img_bytes, caption=f"Image simul√©e : {prompt_image}")
                r√©ponse_finale = "Voici une image simul√©e (version CPU)."

            # Commande : !m√©moire
            elif prompt.lower().startswith("!m√©moire"):
                m√©moire_text = "\n".join(
                    [f"- {m}" for m in st.session_state.memory[-5:]]
                ) or "M√©moire vide."
                r√©ponse_finale = f"Derniers sujets :\n{m√©moire_text}"

            # R√©ponse textuelle
            elif generator:
                try:
                    contexte = " ".join(st.session_state.memory[-2:]) + " " + prompt
                    result = generator(
                        contexte,
                        max_length=100,
                        num_return_sequences=1,
                        temperature=0.8,
                        top_k=50,
                        top_p=0.9,
                        do_sample=True
                    )[0]['generated_text']

                    if result.startswith(prompt):
                        result = result[len(prompt):].strip()

                    r√©ponse_finale = result
                    st.session_state.memory.append(prompt)
                except Exception as e:
                    r√©ponse_finale = f"Erreur pendant la g√©n√©ration : {e}"
            else:
                r√©ponse_finale = "Le mod√®le n‚Äôa pas pu √™tre charg√©."

        st.markdown(r√©ponse_finale)
        st.session_state.messages.append({"role": "assistant", "content": r√©ponse_finale})

# -----------------------------------------------------
# PIED DE PAGE
# -----------------------------------------------------
st.divider()
st.markdown("""
<div style='text-align:center; color:gray; font-size:0.9em;'>
Propuls√© par distilGPT-2 | Compatible Streamlit Cloud | CPU uniquement
</div>
""", unsafe_allow_html=True)    """Cr√©e une image simul√©e (sans √©criture disque, purement en m√©moire)."""
    img = Image.new('RGB', (512, 512), color=(45, 45, 65))
    d = ImageDraw.Draw(img)

    try:
        font = ImageFont.truetype("arial.ttf", 26)
    except IOError:
        font = ImageFont.load_default()

    texte = f"Simulation d'image :\n{prompt_image[:100]}..."
    d.text((20, 230), texte, fill=(255, 255, 120), font=font)

    # Conversion en m√©moire (BytesIO)
    img_bytes = io.BytesIO()
    img.save(img_bytes, format="PNG")
    img_bytes.seek(0)
    return img_bytes

# -----------------------------------------------------
# üí¨ INITIALISATION DU CHAT
# -----------------------------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.memory = []  # m√©moire courte
    st.session_state.messages.append({
        "role": "assistant",
        "content": "üëã Bonjour ! Je suis un assistant IA l√©ger propuls√© par **GPT-2**. "
                   "Demandez-moi de g√©n√©rer du texte, du code, ou tapez `!image votre prompt`."
    })

# -----------------------------------------------------
# üîÑ AFFICHAGE DES MESSAGES
# -----------------------------------------------------
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# -----------------------------------------------------
# üß† SAISIE UTILISATEUR
# -----------------------------------------------------
if prompt := st.chat_input("üí¨ √âcrivez ici pour discuter..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("üí≠ L‚ÄôIA r√©fl√©chit..."):
            r√©ponse_finale = ""

            # üñºÔ∏è Commande : !image
            if prompt.lower().startswith("!image"):
                prompt_image = prompt[6:].strip() or "Aucune description"
                st.info(f"üé® Simulation d'image pour : **{prompt_image}**")

                img_bytes = generer_image(prompt_image)
                st.image(img_bytes, caption=f"Image simul√©e : {prompt_image}")
                r√©ponse_finale = "Voici votre image simul√©e üñºÔ∏è (CPU friendly)."

            # üß† Commande : !m√©moire
            elif prompt.lower().startswith("!m√©moire"):
                m√©moire_text = "\n".join(
                    [f"- {m}" for m in st.session_state.memory[-5:]]
                ) or "M√©moire vide."
                r√©ponse_finale = f"üß† **Derniers sujets :**\n{m√©moire_text}"

            # üí¨ R√©ponse textuelle classique
            else:
                try:
                    contexte = " ".join(st.session_state.memory[-3:]) + " " + prompt
                    result = generator(
                        contexte,
                        max_length=180,
                        num_return_sequences=1,
                        temperature=0.8,
                        top_k=50,
                        top_p=0.9,
                        do_sample=True
                    )[0]['generated_text']

                    # Nettoyer la sortie
                    if result.startswith(prompt):
                        result = result[len(prompt):].strip()

                    r√©ponse_finale = result
                    st.session_state.memory.append(prompt)
                except Exception as e:
                    r√©ponse_finale = f"‚ö†Ô∏è Erreur : {e}"

        st.markdown(r√©ponse_finale)
        st.session_state.messages.append({"role": "assistant", "content": r√©ponse_finale})

# -----------------------------------------------------
# üßæ PIED DE PAGE
# -----------------------------------------------------
st.divider()
st.markdown("""
<div style='text-align:center; color:gray; font-size:0.9em;'>
üöÄ Propuls√© par GPT-2 via ü§ó Transformers | 100 % compatible Streamlit Cloud üåê | Interface am√©lior√©e üí¨
</div>
""", unsafe_allow_html=True)    """Simule la g√©n√©ration d'image (placeholder)."""
    try:
        img = Image.new('RGB', (512, 512), color=(40, 40, 60))
        d = ImageDraw.Draw(img)
        try:
            font = ImageFont.truetype("arial.ttf", 28)
        except IOError:
            font = ImageFont.load_default()
        d.text((20, 230), f"Simulation d'image :\n{prompt_image}", fill=(255, 255, 100), font=font)
        img.save(output_filename)
        return True
    except Exception as e:
        st.error(f"Erreur d'image : {e}")
        return False

# -----------------------------------------------------
# üí¨ INITIALISATION DE LA CONVERSATION
# -----------------------------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.memory = []  # m√©moire courte
    st.session_state.messages.append({
        "role": "assistant",
        "content": "üëã Bonjour ! Je suis un assistant IA l√©ger. "
                   "Tapez une question, une commande `!image`, ou du code √† g√©n√©rer."
    })

# -----------------------------------------------------
# üîÑ AFFICHAGE DES MESSAGES
# -----------------------------------------------------
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# -----------------------------------------------------
# üí° ENTR√âE UTILISATEUR
# -----------------------------------------------------
if prompt := st.chat_input("üí¨ √âcrivez votre message ici..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # -------------------------------------------------
    # üß© G√âN√âRATION DE R√âPONSE
    # -------------------------------------------------
    with st.chat_message("assistant"):
        with st.spinner("L‚ÄôIA r√©fl√©chit..."):
            r√©ponse_finale = ""

            # üñºÔ∏è Commande sp√©ciale : g√©n√©ration d'image simul√©e
            if prompt.lower().startswith("!image"):
                prompt_image = prompt[6:].strip() or "Aucune description fournie"
                filename = f"image_{datetime.datetime.now().strftime('%H%M%S')}.png"
                st.info(f"üé® Simulation d'image pour : **{prompt_image}**")

                if generer_image(prompt_image, filename):
                    st.image(filename, caption=f"Image simul√©e : {prompt_image}")
                    r√©ponse_finale = "Voici votre image simul√©e. (üí° La vraie g√©n√©ration n√©cessite un GPU)"
                    os.remove(filename)
                else:
                    r√©ponse_finale = "‚ö†Ô∏è Impossible de g√©n√©rer l'image simul√©e."

            # üß† Commande sp√©ciale : m√©moire
            elif prompt.lower().startswith("!m√©moire"):
                m√©moire_text = "\n".join([f"- {m}" for m in st.session_state.memory[-5:]]) or "M√©moire vide."
                r√©ponse_finale = f"üß† **M√©moire r√©cente :**\n{m√©moire_text}"

            # üí¨ R√©ponse GPT-2 (texte / code / discussion)
            else:
                try:
                    input_text = " ".join(st.session_state.memory[-3:]) + " " + prompt
                    response = generator(
                        input_text,
                        max_length=200,
                        num_return_sequences=1,
                        do_sample=True,
                        temperature=0.8,
                        top_k=50,
                        top_p=0.95
                    )[0]['generated_text']

                    # Nettoyage
                    if response.startswith(prompt):
                        response = response[len(prompt):].strip()

                    r√©ponse_finale = response
                    st.session_state.memory.append(prompt)  # stocke la m√©moire courte
                except Exception as e:
                    st.error(f"Erreur du mod√®le : {e}")
                    r√©ponse_finale = "‚ùå Erreur de g√©n√©ration de texte."

        st.markdown(r√©ponse_finale)
        st.session_state.messages.append({"role": "assistant", "content": r√©ponse_finale})

# -----------------------------------------------------
# üéõÔ∏è PIED DE PAGE
# -----------------------------------------------------
st.divider()
st.markdown("""
<div style='text-align:center; color:gray; font-size:0.9em;'>
Propuls√© par ü§ó Hugging Face | Con√ßu pour CPU | Interface Streamlit am√©lior√©e ‚ú®
</div>
""", unsafe_allow_html=True)

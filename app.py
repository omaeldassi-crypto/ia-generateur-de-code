import streamlit as st
from transformers import pipeline
from PIL import Image, ImageDraw, ImageFont
import os
import datetime

# -----------------------------------------------------
# âš™ï¸ CONFIGURATION DE LA PAGE
# -----------------------------------------------------
st.set_page_config(
    page_title="ğŸ¤– Assistant Multi-CapacitÃ©s LÃ©ger",
    page_icon="ğŸ§ ",
    layout="wide",
)

# -----------------------------------------------------
# ğŸ§  EN-TÃŠTE DE L'APPLICATION
# -----------------------------------------------------
st.title("ğŸ§  Assistant IA LÃ©ger â€“ GPT-2 Edition")
st.caption("ğŸ’¡ Chatbot capable de **gÃ©nÃ©rer du texte, du code et simuler des images** â€” compatible CPU.")
st.divider()

# -----------------------------------------------------
# ğŸ” CHARGEMENT DU MODÃˆLE GPT-2
# -----------------------------------------------------
@st.cache_resource
def load_generator():
    """Charge le modÃ¨le GPT-2 et le garde en cache."""
    return pipeline("text-generation", model="gpt2")

generator = load_generator()

# -----------------------------------------------------
# ğŸ–¼ï¸ GÃ‰NÃ‰RATION Dâ€™IMAGE SIMULÃ‰E
# -----------------------------------------------------
def generer_image(prompt_image, output_filename):
    """Simule la gÃ©nÃ©ration d'image (placeholder)."""
    try:
        img = Image.new('RGB', (512, 512), color=(40, 40, 60))
        d = ImageDraw.Draw(img)
        try:
            font = ImageFont.truetype("arial.ttf", 28)
        except IOError:
            font = ImageFont.load_default()
        d.text((20, 230), f"Simulation d'image :\n{prompt_image}", fill=(255, 255, 100), font=font)
        img.save(output_filename)
        return True
    except Exception as e:
        st.error(f"Erreur d'image : {e}")
        return False

# -----------------------------------------------------
# ğŸ’¬ INITIALISATION DE LA CONVERSATION
# -----------------------------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.memory = []  # mÃ©moire courte
    st.session_state.messages.append({
        "role": "assistant",
        "content": "ğŸ‘‹ Bonjour ! Je suis un assistant IA lÃ©ger. "
                   "Tapez une question, une commande `!image`, ou du code Ã  gÃ©nÃ©rer."
    })

# -----------------------------------------------------
# ğŸ”„ AFFICHAGE DES MESSAGES
# -----------------------------------------------------
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# -----------------------------------------------------
# ğŸ’¡ ENTRÃ‰E UTILISATEUR
# -----------------------------------------------------
if prompt := st.chat_input("ğŸ’¬ Ã‰crivez votre message ici..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # -------------------------------------------------
    # ğŸ§© GÃ‰NÃ‰RATION DE RÃ‰PONSE
    # -------------------------------------------------
    with st.chat_message("assistant"):
        with st.spinner("Lâ€™IA rÃ©flÃ©chit..."):
            rÃ©ponse_finale = ""

            # ğŸ–¼ï¸ Commande spÃ©ciale : gÃ©nÃ©ration d'image simulÃ©e
            if prompt.lower().startswith("!image"):
                prompt_image = prompt[6:].strip() or "Aucune description fournie"
                filename = f"image_{datetime.datetime.now().strftime('%H%M%S')}.png"
                st.info(f"ğŸ¨ Simulation d'image pour : **{prompt_image}**")

                if generer_image(prompt_image, filename):
                    st.image(filename, caption=f"Image simulÃ©e : {prompt_image}")
                    rÃ©ponse_finale = "Voici votre image simulÃ©e. (ğŸ’¡ La vraie gÃ©nÃ©ration nÃ©cessite un GPU)"
                    os.remove(filename)
                else:
                    rÃ©ponse_finale = "âš ï¸ Impossible de gÃ©nÃ©rer l'image simulÃ©e."

            # ğŸ§  Commande spÃ©ciale : mÃ©moire
            elif prompt.lower().startswith("!mÃ©moire"):
                mÃ©moire_text = "\n".join([f"- {m}" for m in st.session_state.memory[-5:]]) or "MÃ©moire vide."
                rÃ©ponse_finale = f"ğŸ§  **MÃ©moire rÃ©cente :**\n{mÃ©moire_text}"

            # ğŸ’¬ RÃ©ponse GPT-2 (texte / code / discussion)
            else:
                try:
                    input_text = " ".join(st.session_state.memory[-3:]) + " " + prompt
                    response = generator(
                        input_text,
                        max_length=200,
                        num_return_sequences=1,
                        do_sample=True,
                        temperature=0.8,
                        top_k=50,
                        top_p=0.95
                    )[0]['generated_text']

                    # Nettoyage
                    if response.startswith(prompt):
                        response = response[len(prompt):].strip()

                    rÃ©ponse_finale = response
                    st.session_state.memory.append(prompt)  # stocke la mÃ©moire courte
                except Exception as e:
                    st.error(f"Erreur du modÃ¨le : {e}")
                    rÃ©ponse_finale = "âŒ Erreur de gÃ©nÃ©ration de texte."

        st.markdown(rÃ©ponse_finale)
        st.session_state.messages.append({"role": "assistant", "content": rÃ©ponse_finale})

# -----------------------------------------------------
# ğŸ›ï¸ PIED DE PAGE
# -----------------------------------------------------
st.divider()
st.markdown("""
<div style='text-align:center; color:gray; font-size:0.9em;'>
PropulsÃ© par ğŸ¤— Hugging Face | ConÃ§u pour CPU | Interface Streamlit amÃ©liorÃ©e âœ¨
</div>
""", unsafe_allow_html=True)

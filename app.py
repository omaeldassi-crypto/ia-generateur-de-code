import streamlit as st
from transformers import pipeline
from PIL import Image, ImageDraw, ImageFont
import io
import datetime

# -----------------------------------------------------
# âš™ï¸ CONFIGURATION DE LA PAGE
# -----------------------------------------------------
st.set_page_config(
    page_title="ğŸ¤– Assistant IA LÃ©ger (GPT-2)",
    page_icon="ğŸ§ ",
    layout="wide",
)

# -----------------------------------------------------
# ğŸ§  TITRE ET DESCRIPTION
# -----------------------------------------------------
st.title("ğŸ§  Assistant IA LÃ©ger â€“ GPT-2 Edition (CPU Friendly)")
st.caption("ğŸ’¡ Chatbot textuel & simulateur dâ€™images â€“ 100 % compatible Streamlit Cloud (CPU only).")
st.divider()

# -----------------------------------------------------
# ğŸ” CHARGEMENT DU MODÃˆLE GPT-2
# -----------------------------------------------------
@st.cache_resource
def load_generator():
    """Charge le modÃ¨le GPT-2 et le garde en cache pour Ã©viter de le recharger."""
    return pipeline("text-generation", model="gpt2")

generator = load_generator()

# -----------------------------------------------------
# ğŸ–¼ï¸ SIMULATION Dâ€™IMAGE (EN MÃ‰MOIRE)
# -----------------------------------------------------
def generer_image(prompt_image):
    """CrÃ©e une image simulÃ©e (sans Ã©criture disque, purement en mÃ©moire)."""
    img = Image.new('RGB', (512, 512), color=(45, 45, 65))
    d = ImageDraw.Draw(img)

    try:
        font = ImageFont.truetype("arial.ttf", 26)
    except IOError:
        font = ImageFont.load_default()

    texte = f"Simulation d'image :\n{prompt_image[:100]}..."
    d.text((20, 230), texte, fill=(255, 255, 120), font=font)

    # Conversion en mÃ©moire (BytesIO)
    img_bytes = io.BytesIO()
    img.save(img_bytes, format="PNG")
    img_bytes.seek(0)
    return img_bytes

# -----------------------------------------------------
# ğŸ’¬ INITIALISATION DU CHAT
# -----------------------------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.memory = []  # mÃ©moire courte
    st.session_state.messages.append({
        "role": "assistant",
        "content": "ğŸ‘‹ Bonjour ! Je suis un assistant IA lÃ©ger propulsÃ© par **GPT-2**. "
                   "Demandez-moi de gÃ©nÃ©rer du texte, du code, ou tapez `!image votre prompt`."
    })

# -----------------------------------------------------
# ğŸ”„ AFFICHAGE DES MESSAGES
# -----------------------------------------------------
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# -----------------------------------------------------
# ğŸ§  SAISIE UTILISATEUR
# -----------------------------------------------------
if prompt := st.chat_input("ğŸ’¬ Ã‰crivez ici pour discuter..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ğŸ’­ Lâ€™IA rÃ©flÃ©chit..."):
            rÃ©ponse_finale = ""

            # ğŸ–¼ï¸ Commande : !image
            if prompt.lower().startswith("!image"):
                prompt_image = prompt[6:].strip() or "Aucune description"
                st.info(f"ğŸ¨ Simulation d'image pour : **{prompt_image}**")

                img_bytes = generer_image(prompt_image)
                st.image(img_bytes, caption=f"Image simulÃ©e : {prompt_image}")
                rÃ©ponse_finale = "Voici votre image simulÃ©e ğŸ–¼ï¸ (CPU friendly)."

            # ğŸ§  Commande : !mÃ©moire
            elif prompt.lower().startswith("!mÃ©moire"):
                mÃ©moire_text = "\n".join(
                    [f"- {m}" for m in st.session_state.memory[-5:]]
                ) or "MÃ©moire vide."
                rÃ©ponse_finale = f"ğŸ§  **Derniers sujets :**\n{mÃ©moire_text}"

            # ğŸ’¬ RÃ©ponse textuelle classique
            else:
                try:
                    contexte = " ".join(st.session_state.memory[-3:]) + " " + prompt
                    result = generator(
                        contexte,
                        max_length=180,
                        num_return_sequences=1,
                        temperature=0.8,
                        top_k=50,
                        top_p=0.9,
                        do_sample=True
                    )[0]['generated_text']

                    # Nettoyer la sortie
                    if result.startswith(prompt):
                        result = result[len(prompt):].strip()

                    rÃ©ponse_finale = result
                    st.session_state.memory.append(prompt)
                except Exception as e:
                    rÃ©ponse_finale = f"âš ï¸ Erreur : {e}"

        st.markdown(rÃ©ponse_finale)
        st.session_state.messages.append({"role": "assistant", "content": rÃ©ponse_finale})

# -----------------------------------------------------
# ğŸ§¾ PIED DE PAGE
# -----------------------------------------------------
st.divider()
st.markdown("""
<div style='text-align:center; color:gray; font-size:0.9em;'>
ğŸš€ PropulsÃ© par GPT-2 via ğŸ¤— Transformers | 100 % compatible Streamlit Cloud ğŸŒ | Interface amÃ©liorÃ©e ğŸ’¬
</div>
""", unsafe_allow_html=True)    """Simule la gÃ©nÃ©ration d'image (placeholder)."""
    try:
        img = Image.new('RGB', (512, 512), color=(40, 40, 60))
        d = ImageDraw.Draw(img)
        try:
            font = ImageFont.truetype("arial.ttf", 28)
        except IOError:
            font = ImageFont.load_default()
        d.text((20, 230), f"Simulation d'image :\n{prompt_image}", fill=(255, 255, 100), font=font)
        img.save(output_filename)
        return True
    except Exception as e:
        st.error(f"Erreur d'image : {e}")
        return False

# -----------------------------------------------------
# ğŸ’¬ INITIALISATION DE LA CONVERSATION
# -----------------------------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.memory = []  # mÃ©moire courte
    st.session_state.messages.append({
        "role": "assistant",
        "content": "ğŸ‘‹ Bonjour ! Je suis un assistant IA lÃ©ger. "
                   "Tapez une question, une commande `!image`, ou du code Ã  gÃ©nÃ©rer."
    })

# -----------------------------------------------------
# ğŸ”„ AFFICHAGE DES MESSAGES
# -----------------------------------------------------
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# -----------------------------------------------------
# ğŸ’¡ ENTRÃ‰E UTILISATEUR
# -----------------------------------------------------
if prompt := st.chat_input("ğŸ’¬ Ã‰crivez votre message ici..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # -------------------------------------------------
    # ğŸ§© GÃ‰NÃ‰RATION DE RÃ‰PONSE
    # -------------------------------------------------
    with st.chat_message("assistant"):
        with st.spinner("Lâ€™IA rÃ©flÃ©chit..."):
            rÃ©ponse_finale = ""

            # ğŸ–¼ï¸ Commande spÃ©ciale : gÃ©nÃ©ration d'image simulÃ©e
            if prompt.lower().startswith("!image"):
                prompt_image = prompt[6:].strip() or "Aucune description fournie"
                filename = f"image_{datetime.datetime.now().strftime('%H%M%S')}.png"
                st.info(f"ğŸ¨ Simulation d'image pour : **{prompt_image}**")

                if generer_image(prompt_image, filename):
                    st.image(filename, caption=f"Image simulÃ©e : {prompt_image}")
                    rÃ©ponse_finale = "Voici votre image simulÃ©e. (ğŸ’¡ La vraie gÃ©nÃ©ration nÃ©cessite un GPU)"
                    os.remove(filename)
                else:
                    rÃ©ponse_finale = "âš ï¸ Impossible de gÃ©nÃ©rer l'image simulÃ©e."

            # ğŸ§  Commande spÃ©ciale : mÃ©moire
            elif prompt.lower().startswith("!mÃ©moire"):
                mÃ©moire_text = "\n".join([f"- {m}" for m in st.session_state.memory[-5:]]) or "MÃ©moire vide."
                rÃ©ponse_finale = f"ğŸ§  **MÃ©moire rÃ©cente :**\n{mÃ©moire_text}"

            # ğŸ’¬ RÃ©ponse GPT-2 (texte / code / discussion)
            else:
                try:
                    input_text = " ".join(st.session_state.memory[-3:]) + " " + prompt
                    response = generator(
                        input_text,
                        max_length=200,
                        num_return_sequences=1,
                        do_sample=True,
                        temperature=0.8,
                        top_k=50,
                        top_p=0.95
                    )[0]['generated_text']

                    # Nettoyage
                    if response.startswith(prompt):
                        response = response[len(prompt):].strip()

                    rÃ©ponse_finale = response
                    st.session_state.memory.append(prompt)  # stocke la mÃ©moire courte
                except Exception as e:
                    st.error(f"Erreur du modÃ¨le : {e}")
                    rÃ©ponse_finale = "âŒ Erreur de gÃ©nÃ©ration de texte."

        st.markdown(rÃ©ponse_finale)
        st.session_state.messages.append({"role": "assistant", "content": rÃ©ponse_finale})

# -----------------------------------------------------
# ğŸ›ï¸ PIED DE PAGE
# -----------------------------------------------------
st.divider()
st.markdown("""
<div style='text-align:center; color:gray; font-size:0.9em;'>
PropulsÃ© par ğŸ¤— Hugging Face | ConÃ§u pour CPU | Interface Streamlit amÃ©liorÃ©e âœ¨
</div>
""", unsafe_allow_html=True)
